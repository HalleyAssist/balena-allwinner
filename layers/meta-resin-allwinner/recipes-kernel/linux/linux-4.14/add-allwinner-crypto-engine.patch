From 2b8f4ef83619d6021b0f56d7301ed72d6477c81a Mon Sep 17 00:00:00 2001
From: Corentin Labbe <clabbe.montjoie@gmail.com>
Date: Wed, 3 May 2017 17:37:46 +0200
Subject: [PATCH] crypto: Add Allwinner Crypto Engine sun8i-ce

The Crypto Engine is an hardware cryptographic offloader present
on all recent Allwinner SoCs H3, A64, H5, H6
This driver support also the Security System present on A80 and A83T.

This driver supports AES cipher in CTR/CBC/ECB/CTS mode.
This driver supports RSA operations.
This driver provides access to the PRNG of CE/SS.

Signed-off-by: Corentin Labbe <clabbe.montjoie@gmail.com>
---
 drivers/crypto/allwinner/Kconfig                   |  41 +
 drivers/crypto/allwinner/Makefile                  |   1 +
 drivers/crypto/allwinner/sun8i-ce/Makefile         |   4 +
 .../crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c    | 352 ++++++++
 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c  | 899 +++++++++++++++++++++
 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-prng.c  | 152 ++++
 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-rsa.c   | 470 +++++++++++
 drivers/crypto/allwinner/sun8i-ce/sun8i-ce.h       | 332 ++++++++
 8 files changed, 2251 insertions(+)
 create mode 100644 drivers/crypto/allwinner/Makefile
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/Makefile
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-prng.c
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/sun8i-ce-rsa.c
 create mode 100644 drivers/crypto/allwinner/sun8i-ce/sun8i-ce.h

diff --git a/drivers/crypto/allwinner/Kconfig b/drivers/crypto/allwinner/Kconfig
index 0768e27c0c74f..c3ee43a5571c6 100644
--- a/drivers/crypto/allwinner/Kconfig
+++ b/drivers/crypto/allwinner/Kconfig
@@ -3,3 +3,44 @@ config CRYPTO_DEV_ALLWINNER
 	default y if ARCH_SUNXI
 	help
 	  Say Y here to get to see options for Allwinner hardware crypto devices
+
+config CRYPTO_DEV_SUN8I_CE
+	tristate "Support for Allwinner Crypto Engine cryptographic accelerator"
+	select CRYPTO_BLKCIPHER
+	select CRYPTO_ENGINE
+	select CRYPTO_ECB
+	select CRYPTO_CBC
+	select CRYPTO_AES
+	select CRYPTO_DES
+	depends on CRYPTO_DEV_ALLWINNER
+	help
+	  Select y here for having support for the crypto Engine availlable on
+	  Allwinner SoC H3 and A64.
+	  The Crypto Engine handle AES/3DES ciphers in ECB/CBC mode.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called sun8i-ce.
+
+config CRYPTO_DEV_SUN8I_CE_DEBUG
+	bool "Enabled sun8i-ce stats"
+	depends on CRYPTO_DEV_SUN8I_CE
+	depends on DEBUG_FS
+	help
+	  Say y to enabled sun8i-ce debug stats.
+	  This will create /sys/kernel/debug/sun8i-ce/stats for displaying
+	  the number of requests per flow and per algorithm.
+
+config CRYPTO_DEV_SUN8I_CE_PRNG
+	bool "Support for sun8i Allwinner Crypto Engine PRNG"
+	depends on CRYPTO_DEV_SUN8I_CE
+	help
+	  This driver provides kernel-side support for the Pseudo-Random
+	  Number Generator found in the sun8i Security System.
+
+config CRYPTO_DEV_SUN8I_CE_RSA
+       bool "Support for sun8i Allwinner Security System RSA"
+       depends on CRYPTO_DEV_SUN8I_CE
+       select CRYPTO_RSA
+       help
+         This driver provides kernel-side support for the RSA TODO
+         found in the sun8i Security System.
diff --git a/drivers/crypto/allwinner/Makefile b/drivers/crypto/allwinner/Makefile
new file mode 100644
index 0000000000000..11f02db9ee06d
--- /dev/null
+++ b/drivers/crypto/allwinner/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_CRYPTO_DEV_SUN8I_CE) += sun8i-ce/
diff --git a/drivers/crypto/allwinner/sun8i-ce/Makefile b/drivers/crypto/allwinner/sun8i-ce/Makefile
new file mode 100644
index 0000000000000..54524c6358c56
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/Makefile
@@ -0,0 +1,4 @@
+obj-$(CONFIG_CRYPTO_DEV_SUN8I_CE) += sun8i-ce.o
+sun8i-ce-y += sun8i-ce-core.o sun8i-ce-cipher.o
+sun8i-ce-$(CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG) += sun8i-ce-prng.o
+sun8i-ce-$(CONFIG_CRYPTO_DEV_SUN8I_CE_RSA) += sun8i-ce-rsa.o
diff --git a/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
new file mode 100644
index 0000000000000..1dab3ab4ad9a3
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
@@ -0,0 +1,352 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * sun8i-ce-cipher.c - hardware cryptographic accelerator for
+ * Allwinner H3/A64/H5/H2+/H6/A80/A83T SoC
+ *
+ * Copyright (C) 2016-2018 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for AES cipher with 128,192,256 bits keysize in
+ * CBC and ECB mode.
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ */
+
+#include <linux/crypto.h>
+#include <linux/io.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/dma-mapping.h>
+#include <crypto/internal/skcipher.h>
+#include "sun8i-ce.h"
+
+static int sun8i_ce_cipher(struct skcipher_request *areq)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+	struct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);
+	int flow = ss->flow;
+	struct ce_task *cet;
+	int nr_sgs, nr_sgd;
+	struct scatterlist *sg;
+	struct scatterlist *in_sg = areq->src;
+	struct scatterlist *out_sg = areq->dst;
+	int i;
+	int chunked_src, chunked_dst;
+	int err = 0;
+	unsigned int todo, len;
+	struct skcipher_alg *alg = crypto_skcipher_alg(tfm);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.skcipher);
+
+	dev_dbg(ss->dev, "%s %s %u %x IV(%p %u) key=%u\n", __func__,
+		crypto_tfm_alg_name(areq->base.tfm),
+		areq->cryptlen,
+		rctx->op_dir, areq->iv, crypto_skcipher_ivsize(tfm),
+		op->keylen);
+
+	chunked_src = 1;
+	sg = areq->src;
+	while (sg && chunked_src == 1) {
+		if ((sg->length % 4) != 0)
+			chunked_src = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32)))
+			chunked_src = 0;
+		sg = sg_next(sg);
+	}
+	chunked_dst = 1;
+	sg = areq->dst;
+	while (sg && chunked_dst == 1) {
+		if ((sg->length % 4) != 0)
+			chunked_dst = 0;
+		if (!IS_ALIGNED(sg->offset, sizeof(u32)))
+			chunked_dst = 0;
+		sg = sg_next(sg);
+	}
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	algt->stat_req++;
+#endif
+
+	/* on SS, src and dst SG must have the same len TODO */
+
+	if (chunked_src == 0 || chunked_dst == 0 || sg_nents(in_sg) > 8) {
+		SKCIPHER_REQUEST_ON_STACK(req, op->fallback_tfm);
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+		algt->stat_fb++;
+#endif
+		skcipher_request_set_tfm(req, op->fallback_tfm);
+		skcipher_request_set_callback(req, areq->base.flags, NULL,
+					      NULL);
+		skcipher_request_set_crypt(req, areq->src, areq->dst,
+					   areq->cryptlen, areq->iv);
+		if (rctx->op_dir & CE_DECRYPTION)
+			err = crypto_skcipher_decrypt(req);
+		else
+			err = crypto_skcipher_encrypt(req);
+		skcipher_request_zero(req);
+		return err;
+	}
+
+	flow = rctx->flow;
+
+	mutex_lock(&ss->chanlist[flow].lock);
+
+	cet = ss->chanlist[flow].tl;
+	memset(cet, 0, sizeof(struct ce_task));
+
+	cet->t_id = flow;
+	cet->t_common_ctl = ss->variant->alg_cipher[algt->ce_algo_id];
+	cet->t_common_ctl |= rctx->op_dir | BIT(31);
+	cet->t_dlen = areq->cryptlen / 4;
+	if (algt->ce_blockmode == CE_ID_OP_CTS)
+		cet->t_dlen = areq->cryptlen;
+
+	cet->t_sym_ctl = ss->variant->op_mode[algt->ce_blockmode];
+	cet->t_sym_ctl |= op->keymode;
+	if (algt->ce_blockmode == CE_ID_OP_CTR)
+		cet->t_sym_ctl |= CE_CTR_128;
+	if (algt->ce_blockmode == CE_ID_OP_CTS)
+		cet->t_sym_ctl |= CE_CTS;
+	cet->t_asym_ctl = 0;
+
+	ss->chanlist[flow].op_mode = ss->variant->op_mode[algt->ce_blockmode];
+	ss->chanlist[flow].op_dir = rctx->op_dir;
+	ss->chanlist[flow].method = ss->variant->alg_cipher[algt->ce_algo_id];
+	ss->chanlist[flow].keylen = op->keylen;
+
+	cet->t_key = dma_map_single(ss->dev, op->key, op->keylen,
+				    DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_key)) {
+		dev_err(ss->dev, "Cannot DMA MAP KEY\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	if (areq->iv) {
+		ss->chanlist[flow].ivlen = crypto_skcipher_ivsize(tfm);
+		ss->chanlist[flow].bounce_iv = kzalloc(ss->chanlist[flow].ivlen,
+						       GFP_KERNEL | GFP_DMA);
+		if (!ss->chanlist[flow].bounce_iv) {
+			err = -ENOMEM;
+			goto theend;
+		}
+		memcpy(ss->chanlist[flow].bounce_iv, areq->iv,
+		       crypto_skcipher_ivsize(tfm));
+		ss->chanlist[flow].next_iv = kzalloc(ss->chanlist[flow].ivlen,
+						     GFP_KERNEL | GFP_DMA);
+		if (!ss->chanlist[flow].next_iv) {
+			err = -ENOMEM;
+			goto theend;
+		}
+	}
+
+	if (in_sg == out_sg) {
+		nr_sgs = dma_map_sg(ss->dev, in_sg, sg_nents(in_sg),
+				    DMA_BIDIRECTIONAL);
+		if (nr_sgs < 0 || nr_sgs > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgs);
+			err = -EINVAL;
+			goto theend;
+		}
+		nr_sgd = nr_sgs;
+	} else {
+		nr_sgs = dma_map_sg(ss->dev, in_sg, sg_nents(in_sg),
+				    DMA_TO_DEVICE);
+		if (nr_sgs < 0 || nr_sgs > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgs);
+			err = -EINVAL;
+			goto theend;
+		}
+		nr_sgd = dma_map_sg(ss->dev, out_sg, sg_nents(out_sg),
+				    DMA_FROM_DEVICE);
+		if (nr_sgd < 0 || nr_sgd > 8) {
+			dev_info(ss->dev, "Invalid sg number %d\n", nr_sgd);
+			err = -EINVAL;
+			goto theend;
+		}
+	}
+
+	len = areq->cryptlen;
+	for_each_sg(in_sg, sg, nr_sgs, i) {
+		cet->t_src[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_src[i].len = todo / 4;
+		len -= todo;
+	}
+
+	len = areq->cryptlen;
+	for_each_sg(out_sg, sg, nr_sgd, i) {
+		cet->t_dst[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_dst[i].len = todo / 4;
+		len -= todo;
+	}
+
+	err = sun8i_ce_run_task(ss, flow, "cipher");
+
+	if (areq->iv) {
+		memcpy(areq->iv, ss->chanlist[flow].next_iv,
+		       ss->chanlist[flow].ivlen);
+		memzero_explicit(ss->chanlist[flow].bounce_iv,
+				 ss->chanlist[flow].ivlen);
+		kfree(ss->chanlist[flow].bounce_iv);
+		kfree(ss->chanlist[flow].next_iv);
+		ss->chanlist[flow].bounce_iv = NULL;
+		ss->chanlist[flow].next_iv = NULL;
+	}
+
+	dma_unmap_single(ss->dev, cet->t_key, op->keylen, DMA_TO_DEVICE);
+	if (in_sg == out_sg) {
+		dma_unmap_sg(ss->dev, in_sg, nr_sgs, DMA_BIDIRECTIONAL);
+	} else {
+		dma_unmap_sg(ss->dev, in_sg, nr_sgs, DMA_TO_DEVICE);
+		dma_unmap_sg(ss->dev, out_sg, nr_sgd, DMA_FROM_DEVICE);
+	}
+
+theend:
+	mutex_unlock(&ss->chanlist[flow].lock);
+
+	return err;
+}
+
+static int handle_cipher_request(struct crypto_engine *engine,
+				 void *areq)
+{
+	int err;
+	struct skcipher_request *breq = container_of(areq, struct skcipher_request, base);
+
+	err = sun8i_ce_cipher(breq);
+	crypto_finalize_skcipher_request(engine, breq, err);
+
+	return 0;
+}
+
+int sun8i_ce_skdecrypt(struct skcipher_request *areq)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);
+	int e = get_engine_number(op->ss);
+	struct crypto_engine *engine = op->ss->chanlist[e].engine;
+
+	rctx->op_dir = CE_DECRYPTION;
+	rctx->flow = e;
+
+	return crypto_transfer_skcipher_request_to_engine(engine, areq);
+}
+
+int sun8i_ce_skencrypt(struct skcipher_request *areq)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);
+	struct sun8i_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);
+	int e = get_engine_number(op->ss);
+	struct crypto_engine *engine = op->ss->chanlist[e].engine;
+
+	rctx->op_dir = CE_ENCRYPTION;
+	rctx->flow = e;
+
+	return crypto_transfer_skcipher_request_to_engine(engine, areq);
+}
+
+int sun8i_ce_cipher_init(struct crypto_tfm *tfm)
+{
+	struct sun8i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+	struct sun8i_ss_alg_template *algt;
+	const char *name = crypto_tfm_alg_name(tfm);
+	struct crypto_skcipher *sktfm = __crypto_skcipher_cast(tfm);
+	struct skcipher_alg *alg = crypto_skcipher_alg(sktfm);
+
+	memset(op, 0, sizeof(struct sun8i_tfm_ctx));
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.skcipher);
+	op->ss = algt->ss;
+
+	sktfm->reqsize = sizeof(struct sun8i_cipher_req_ctx);
+
+	op->fallback_tfm = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_ASYNC |
+						 CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(op->fallback_tfm)) {
+		dev_err(op->ss->dev, "ERROR: Cannot allocate fallback for %s %ld\n",
+			name, PTR_ERR(op->fallback_tfm));
+		return PTR_ERR(op->fallback_tfm);
+	}
+
+	op->enginectx.op.do_one_request = handle_cipher_request;
+	op->enginectx.op.prepare_request = NULL;
+	op->enginectx.op.unprepare_request = NULL;
+
+	return 0;
+}
+
+void sun8i_ce_cipher_exit(struct crypto_tfm *tfm)
+{
+	struct sun8i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+
+	if (op->key) {
+		memzero_explicit(op->key, op->keylen);
+		kfree(op->key);
+	}
+	crypto_free_skcipher(op->fallback_tfm);
+}
+
+int sun8i_ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			unsigned int keylen)
+{
+	struct sun8i_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+
+	switch (keylen) {
+	case 128 / 8:
+		op->keymode = CE_AES_128BITS;
+		break;
+	case 192 / 8:
+		op->keymode = CE_AES_192BITS;
+		break;
+	case 256 / 8:
+		op->keymode = CE_AES_256BITS;
+		break;
+	default:
+		dev_err(ss->dev, "ERROR: Invalid keylen %u\n", keylen);
+		crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+	if (op->key) {
+		memzero_explicit(op->key, op->keylen);
+		kfree(op->key);
+	}
+	op->keylen = keylen;
+	op->key = kmalloc(keylen, GFP_KERNEL | GFP_DMA);
+	if (!op->key)
+		return -ENOMEM;
+	memcpy(op->key, key, keylen);
+
+	return crypto_skcipher_setkey(op->fallback_tfm, key, keylen);
+}
+
+int sun8i_ce_des3_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			 unsigned int keylen)
+{
+	struct sun8i_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct sun8i_ss_ctx *ss = op->ss;
+
+	if (unlikely(keylen != 3 * DES_KEY_SIZE)) {
+		dev_err(ss->dev, "Invalid keylen %u\n", keylen);
+		crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+
+	if (op->key) {
+		memzero_explicit(op->key, op->keylen);
+		kfree(op->key);
+	}
+	op->keylen = keylen;
+	op->key = kmalloc(keylen, GFP_KERNEL | GFP_DMA);
+	if (!op->key)
+		return -ENOMEM;
+	memcpy(op->key, key, keylen);
+
+	return crypto_skcipher_setkey(op->fallback_tfm, key, keylen);
+}
diff --git a/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
new file mode 100644
index 0000000000000..41fd11f333bed
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
@@ -0,0 +1,899 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * sun8i-ce-core.c - hardware cryptographic accelerator for
+ * Allwinner H3/A64/H5/H2+/H6/A80/A83T SoC
+ *
+ * Copyright (C) 2015-2018 Corentin Labbe <clabbe.montjoie@gmail.com>
+ *
+ * Core file which registers crypto algorithms supported by the CryptoEngine.
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ */
+#include <linux/clk.h>
+#include <linux/crypto.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/scatterlist.h>
+#include <crypto/scatterwalk.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/akcipher.h>
+#include <crypto/internal/skcipher.h>
+#include <linux/dma-mapping.h>
+
+#include "sun8i-ce.h"
+
+static const struct ce_variant ce_h3_variant = {
+	.alg_cipher = { CE_ID_NOTSUPP, CE_ALG_AES, CE_ALG_DES, CE_ALG_3DES, },
+	.op_mode = { CE_ID_NOTSUPP, CE_OP_ECB, CE_OP_CBC, CE_OP_CTR,
+		CE_OP_CTS, CE_ID_NOTSUPP, CE_ID_NOTSUPP, CE_ID_NOTSUPP
+	},
+	.intreg = CE_ISR,
+	.maxflow = 4,
+	.prng = CE_ALG_PRNG,
+	.maxrsakeysize = 2048,
+	.rsa_op_mode = { CE_OP_RSA_512, CE_OP_RSA_1024, CE_OP_RSA_2048,
+			CE_OP_RSA_3072, CE_OP_RSA_4096, },
+	.alg_akcipher = { CE_ID_NOTSUPP, CE_ALG_RSA, },
+};
+
+static const struct ce_variant ce_h5_variant = {
+	.alg_cipher = { CE_ID_NOTSUPP, CE_ALG_AES, CE_ALG_DES, CE_ALG_3DES, },
+	.op_mode = { CE_ID_NOTSUPP, CE_OP_ECB, CE_OP_CBC, CE_OP_CTR,
+		CE_OP_CTS, CE_ID_NOTSUPP, CE_ID_NOTSUPP, CE_ID_NOTSUPP
+	},
+	.intreg = CE_ISR,
+	.maxflow = 4,
+	.prng = CE_ALG_PRNG,
+	.maxrsakeysize = 4096,
+	.rsa_op_mode = { CE_OP_RSA_512, CE_OP_RSA_1024, CE_OP_RSA_2048,
+			CE_OP_RSA_3072, CE_OP_RSA_4096, },
+	.alg_akcipher = { CE_ID_NOTSUPP, CE_ALG_RSA, },
+};
+
+static const struct ce_variant ce_a64_variant = {
+	.alg_cipher = { CE_ID_NOTSUPP, CE_ALG_AES, CE_ALG_DES, CE_ALG_3DES, },
+	.op_mode = { CE_ID_NOTSUPP, CE_OP_ECB, CE_OP_CBC, CE_OP_CTR,
+		CE_OP_CTS, CE_ID_NOTSUPP, CE_ID_NOTSUPP, CE_ID_NOTSUPP
+	},
+	.intreg = CE_ISR,
+	.maxflow = 4,
+	.prng = CE_ALG_PRNG,
+	.maxrsakeysize = 2048,
+	.rsa_op_mode = { CE_OP_RSA_512, CE_OP_RSA_1024, CE_OP_RSA_2048,
+			CE_ID_NOTSUPP, CE_ID_NOTSUPP, },
+	.alg_akcipher = { CE_ID_NOTSUPP, CE_ALG_RSA, },
+};
+
+static const struct ce_variant ce_a83t_variant = {
+	.alg_cipher = { CE_ID_NOTSUPP, SS_ALG_AES, SS_ALG_DES, SS_ALG_3DES, },
+	.op_mode = { CE_ID_NOTSUPP, SS_OP_ECB, SS_OP_CBC, SS_OP_CTR,
+		CE_ID_NOTSUPP, CE_ID_NOTSUPP, CE_ID_NOTSUPP, CE_ID_NOTSUPP
+	},
+	.is_ss = true,
+	.intreg = SS_INT_STA_REG,
+	.maxflow = 2,
+	.prng = SS_ALG_PRNG,
+	.maxrsakeysize = 3072,
+	.rsa_op_mode = { SS_OP_RSA_512, SS_OP_RSA_1024, SS_OP_RSA_2048,
+			CE_ID_NOTSUPP, CE_ID_NOTSUPP, },
+	.alg_akcipher = { CE_ID_NOTSUPP, SS_ALG_RSA, },
+};
+
+int get_engine_number(struct sun8i_ss_ctx *ss)
+{
+	int e = ss->flow;
+
+	ss->flow++;
+	if (ss->flow >= ss->variant->maxflow)
+		ss->flow = 0;
+
+	return e;
+}
+
+static int sun8i_ss_run_task(struct sun8i_ss_ctx *ss, int flow,
+			     const char *name)
+{
+	int err = 0;
+	u32 v = 1;
+	struct ce_task *cet = ss->chanlist[flow].tl;
+	int i;
+	u32 *iv;
+
+	mutex_lock(&ss->mlock);
+	/* choose between stream0/stream1 */
+	if (flow)
+		v |= SS_FLOW1;
+	else
+		v |= SS_FLOW0;
+
+	v |= ss->chanlist[flow].op_mode;
+	v |= ss->chanlist[flow].method;
+
+	/* dir bit is different on SS */
+	if (ss->chanlist[flow].op_dir)
+		v |= SS_DECRYPTION;
+
+	if (ss->chanlist[flow].method == SS_ALG_PRNG) {
+		/* grab continue mode */
+		v |= SS_RNG_CONTINUE;
+		/* TODO */
+		cet->t_src[0].len = 5;
+	}
+
+	switch (ss->chanlist[flow].keylen) {
+	case 128 / 8:
+		v |= CE_AES_128BITS << 7;
+	break;
+	case 192 / 8:
+		v |= CE_AES_192BITS << 7;
+	break;
+	case 256 / 8:
+		v |= CE_AES_256BITS << 7;
+	break;
+	}
+
+	/* enable INT for this flow */
+	writel(BIT(flow), ss->base + SS_INT_CTL_REG);
+
+	if (cet->t_key)
+		writel(cet->t_key, ss->base + SS_KEY_ADR_REG);
+
+	/* hash arbitrary IV */
+	if (cet->t_common_ctl & BIT(16)) {
+		v |= BIT(17);
+		dev_info(ss->dev, "Need to set IV from %p\n",
+			 ss->chanlist[flow].bounce_iv);
+		writel(cet->t_iv, ss->base + SS_KEY_ADR_REG);
+	}
+
+	/* For PRNG the IV is set ... in key :) */
+	if (ss->chanlist[flow].method == SS_ALG_PRNG)
+		writel(cet->t_iv, ss->base + SS_KEY_ADR_REG);
+	else
+		writel(cet->t_iv, ss->base + SS_IV_ADR_REG);
+
+	for (i = 0; i < MAX_SG; i++) {
+		if (!cet->t_dst[i].addr)
+			break;
+		dev_info(ss->dev,
+			 "Processing SG %d %s ctl=%x %d to %d method=%x op=%x opdir=%x\n",
+			 i, name, v,
+			 cet->t_src[i].len, cet->t_dst[i].len,
+			 ss->chanlist[flow].method,
+			 ss->chanlist[flow].op_mode,
+			 ss->chanlist[flow].op_dir);
+
+		writel(cet->t_src[i].addr, ss->base + SS_SRC_ADR_REG);
+		writel(cet->t_dst[i].addr, ss->base + SS_DST_ADR_REG);
+		writel(cet->t_src[i].len, ss->base + SS_LEN_ADR_REG);
+
+		reinit_completion(&ss->chanlist[flow].complete);
+		ss->chanlist[flow].status = 0;
+		wmb();
+
+		writel(v, ss->base + SS_CTL_REG);
+		wait_for_completion_interruptible_timeout(&ss->chanlist[flow].complete,
+				msecs_to_jiffies(2000));
+		if (ss->chanlist[flow].status == 0) {
+			dev_err(ss->dev, "DMA timeout for %s\n", name);
+			err = -EINVAL;
+			goto theend;
+		}
+		/*print_hex_dump(KERN_INFO, "IV ", DUMP_PREFIX_NONE, 16, 1, ss->chanlist[flow].bounce_iv,
+			ss->chanlist[flow].ivlen, false);*/
+
+	}
+	/* copy next IV */
+	if (ss->chanlist[flow].next_iv) {
+		iv = ss->chanlist[flow].next_iv;
+		for (i = 0; i < 4; i++) {
+			if (flow)
+				*iv = readl(ss->base + SS_CTR_REG1 + i * 4);
+			else
+				*iv = readl(ss->base + SS_CTR_REG0 + i * 4);
+			iv++;
+		}
+	}
+theend:
+	mutex_unlock(&ss->mlock);
+
+	return err;
+}
+
+int sun8i_ce_run_task(struct sun8i_ss_ctx *ss, int flow, const char *name)
+{
+	u32 v;
+	int err = 0;
+	struct ce_task *cet = ss->chanlist[flow].tl;
+
+	if (ss->chanlist[flow].bounce_iv) {
+		cet->t_iv = dma_map_single(ss->dev,
+					   ss->chanlist[flow].bounce_iv,
+					   ss->chanlist[flow].ivlen,
+					   DMA_BIDIRECTIONAL);
+		if (dma_mapping_error(ss->dev, cet->t_iv)) {
+			dev_err(ss->dev, "Cannot DMA MAP IV\n");
+			return -EFAULT;
+		}
+	}
+	if (ss->chanlist[flow].next_iv) {
+		cet->t_ctr = dma_map_single(ss->dev,
+					    ss->chanlist[flow].next_iv,
+					    ss->chanlist[flow].ivlen,
+					    DMA_FROM_DEVICE);
+		if (dma_mapping_error(ss->dev, cet->t_ctr)) {
+			dev_err(ss->dev, "Cannot DMA MAP IV\n");
+			err = -EFAULT;
+			goto err_next_iv;
+		}
+	}
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	ss->chanlist[flow].stat_req++;
+#endif
+
+	if (ss->variant->is_ss) {
+		err = sun8i_ss_run_task(ss, flow, name);
+	} else {
+		mutex_lock(&ss->mlock);
+
+		v = readl(ss->base + CE_ICR);
+		v |= 1 << flow;
+		writel(v, ss->base + CE_ICR);
+
+		reinit_completion(&ss->chanlist[flow].complete);
+		writel(ss->chanlist[flow].t_phy, ss->base + CE_TDQ);
+
+		ss->chanlist[flow].status = 0;
+		/* Be sure all data is written before enabling the task */
+		wmb();
+
+		writel(1, ss->base + CE_TLR);
+		mutex_unlock(&ss->mlock);
+
+		wait_for_completion_interruptible_timeout(&ss->chanlist[flow].complete,
+							  msecs_to_jiffies(5000));
+
+		if (ss->chanlist[flow].status == 0) {
+			dev_err(ss->dev, "DMA timeout for %s\n", name);
+			err = -EINVAL;
+		}
+		/* No need to lock for this read, the channel is locked so
+		 * nothing could modify the error value for this channel
+		 */
+		v = readl(ss->base + CE_ESR);
+		if (v) {
+			dev_err(ss->dev, "CE ERROR %x for flow %x\n", v, flow);
+			err = -EFAULT;
+			v >>= (flow * 4);
+			switch (v) {
+			case 1:
+				dev_err(ss->dev, "CE ERROR: algorithm not supported\n");
+			break;
+			case 2:
+				dev_err(ss->dev, "CE ERROR: data length error\n");
+			break;
+			case 4:
+				dev_err(ss->dev, "CE ERROR: keysram access error for AES\n");
+			break;
+			default:
+				dev_err(ss->dev, "CE ERROR: invalid error\n");
+			}
+		}
+	}
+
+	if (ss->chanlist[flow].next_iv) {
+		dma_unmap_single(ss->dev, cet->t_ctr,
+				 ss->chanlist[flow].ivlen,
+				 DMA_FROM_DEVICE);
+	}
+err_next_iv:
+	if (ss->chanlist[flow].bounce_iv) {
+		dma_unmap_single(ss->dev, cet->t_iv,
+				 ss->chanlist[flow].ivlen,
+				 DMA_BIDIRECTIONAL);
+	}
+
+	return err;
+}
+
+static irqreturn_t ce_irq_handler(int irq, void *data)
+{
+	u32 p;
+	struct sun8i_ss_ctx *ss = (struct sun8i_ss_ctx *)data;
+	int flow = 0;
+
+	p = readl(ss->base + ss->variant->intreg);
+	for (flow = 0; flow < ss->variant->maxflow; flow++) {
+		if (p & (BIT(flow))) {
+			writel(BIT(flow), ss->base + ss->variant->intreg);
+			ss->chanlist[flow].status = 1;
+			complete(&ss->chanlist[flow].complete);
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static struct sun8i_ss_alg_template ce_algs[] = {
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_AES,
+	.ce_blockmode = CE_ID_OP_CTR,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "ctr(aes)",
+			.cra_driver_name = "ctr-aes-sun8i-ce",
+			.cra_priority = 300,
+			.cra_blocksize = AES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= sun8i_ce_aes_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_AES,
+	.ce_blockmode = CE_ID_OP_CTS,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cts(cbc(aes))",
+			.cra_driver_name = "cts(cbc-aes-sun8i-ce)",
+			.cra_priority = 300,
+			.cra_blocksize = AES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= sun8i_ce_aes_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_AES,
+	.ce_blockmode = CE_ID_OP_CBC,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cbc(aes)",
+			.cra_driver_name = "cbc-aes-sun8i-ce",
+			.cra_priority = 300,
+			.cra_blocksize = AES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= sun8i_ce_aes_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_AES,
+	.ce_blockmode = CE_ID_OP_ECB,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "ecb(aes)",
+			.cra_driver_name = "ecb-aes-sun8i-ce",
+			.cra_priority = 300,
+			.cra_blocksize = AES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= sun8i_ce_aes_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_DES3,
+	.ce_blockmode = CE_ID_OP_CBC,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cbc(des3_ede)",
+			.cra_driver_name = "cbc-des3-sun8i-ce",
+			.cra_priority = 300,
+			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= DES3_EDE_KEY_SIZE,
+		.max_keysize	= DES3_EDE_KEY_SIZE,
+		.ivsize		= DES3_EDE_BLOCK_SIZE,
+		.setkey		= sun8i_ce_des3_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.ce_algo_id = CE_ID_CIPHER_DES3,
+	.ce_blockmode = CE_ID_OP_ECB,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "ecb(des3_ede)",
+			.cra_driver_name = "ecb-des3-sun8i-ce",
+			.cra_priority = 300,
+			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+			.cra_init = sun8i_ce_cipher_init,
+			.cra_exit = sun8i_ce_cipher_exit,
+		},
+		.min_keysize	= DES3_EDE_KEY_SIZE,
+		.max_keysize	= DES3_EDE_KEY_SIZE,
+		.ivsize		= DES3_EDE_BLOCK_SIZE,
+		.setkey		= sun8i_ce_des3_setkey,
+		.encrypt	= sun8i_ce_skencrypt,
+		.decrypt	= sun8i_ce_skdecrypt,
+	}
+},
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+{
+	.type = CRYPTO_ALG_TYPE_RNG,
+	.alg.rng = {
+		.base = {
+			.cra_name		= "stdrng",
+			.cra_driver_name	= "sun8i_ce_rng",
+			.cra_priority		= 100,
+			.cra_ctxsize		= sizeof(struct sun8i_ce_prng_ctx),
+			.cra_module		= THIS_MODULE,
+			.cra_init		= sun8i_ce_prng_init,
+		},
+		.generate               = sun8i_ce_prng_generate,
+		.seed                   = sun8i_ce_prng_seed,
+		.seedsize               = PRNG_SEED_SIZE,
+	}
+},
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+{
+	.type = CRYPTO_ALG_TYPE_AKCIPHER,
+	.ce_algo_id = CE_ID_AKCIPHER_RSA,
+	.alg.rsa = {
+		.encrypt = sun8i_rsa_encrypt,
+		.decrypt = sun8i_rsa_decrypt,
+		.sign = sun8i_rsa_sign,
+		.verify = sun8i_rsa_verify,
+		.set_priv_key = sun8i_rsa_set_priv_key,
+		.set_pub_key = sun8i_rsa_set_pub_key,
+		.max_size = sun8i_rsa_max_size,
+		.init = sun8i_rsa_init,
+		.exit = sun8i_rsa_exit,
+		.base = {
+			.cra_name = "rsa",
+			.cra_driver_name = "rsa-sun8i-ce",
+			.cra_priority = 300,
+			.cra_flags = CRYPTO_ALG_TYPE_AKCIPHER |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sun8i_tfm_rsa_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 3,
+		}
+	}
+},
+#endif
+};
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+static int sun8i_ce_dbgfs_read(struct seq_file *seq, void *v)
+{
+	struct sun8i_ss_ctx *ss = seq->private;
+	int i;
+
+	for (i = 0; i < ss->variant->maxflow; i++) {
+		seq_printf(seq, "Channel %d: req %lu\n", i, ss->chanlist[i].stat_req);
+	}
+	for (i = 0; i < ARRAY_SIZE(ce_algs); i++) {
+		ce_algs[i].ss = ss;
+		switch (ce_algs[i].type) {
+		case CRYPTO_ALG_TYPE_SKCIPHER:
+			seq_printf(seq, "%s %s %lu %lu\n",
+				   ce_algs[i].alg.skcipher.base.cra_driver_name,
+				   ce_algs[i].alg.skcipher.base.cra_name,
+				   ce_algs[i].stat_req, ce_algs[i].stat_fb);
+			break;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+		case CRYPTO_ALG_TYPE_RNG:
+			seq_printf(seq, "%s %s %lu %lu\n",
+				   ce_algs[i].alg.rng.base.cra_driver_name,
+				   ce_algs[i].alg.rng.base.cra_name,
+				   ce_algs[i].stat_req, ce_algs[i].stat_fb);
+			break;
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+		case CRYPTO_ALG_TYPE_AKCIPHER:
+			seq_printf(seq, "%s %s %lu %lu\n",
+				   ce_algs[i].alg.rsa.base.cra_driver_name,
+				   ce_algs[i].alg.rsa.base.cra_name,
+				   ce_algs[i].stat_req, ce_algs[i].stat_fb);
+			break;
+#endif
+		}
+	}
+	return 0;
+}
+
+static int sun8i_ce_dbgfs_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sun8i_ce_dbgfs_read, inode->i_private);
+}
+
+static const struct file_operations sun8i_ce_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = sun8i_ce_dbgfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+#endif
+
+static int sun8i_ce_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	u32 v;
+	int err, i, ce_method, id;
+	struct sun8i_ss_ctx *ss;
+
+	if (!pdev->dev.of_node)
+		return -ENODEV;
+
+	ss = devm_kzalloc(&pdev->dev, sizeof(*ss), GFP_KERNEL);
+	if (!ss)
+		return -ENOMEM;
+
+	ss->variant = of_device_get_match_data(&pdev->dev);
+	if (!ss->variant) {
+		dev_err(&pdev->dev, "Missing Crypto Engine variant\n");
+		return -EINVAL;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	ss->base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(ss->base)) {
+		err = PTR_ERR(ss->base);
+		dev_err(&pdev->dev, "Cannot request MMIO %d\n", err);
+		return err;
+	}
+
+	ss->busclk = devm_clk_get(&pdev->dev, "ahb1_ce");
+	if (IS_ERR(ss->busclk)) {
+		err = PTR_ERR(ss->busclk);
+		dev_err(&pdev->dev, "Cannot get AHB SS clock err=%d\n", err);
+		return err;
+	}
+	dev_dbg(&pdev->dev, "clock ahb_ss acquired\n");
+
+	ss->ssclk = devm_clk_get(&pdev->dev, "mod");
+	if (IS_ERR(ss->ssclk)) {
+		err = PTR_ERR(ss->ssclk);
+		dev_err(&pdev->dev, "Cannot get SS clock err=%d\n", err);
+		return err;
+	}
+
+	/* Get Non Secure IRQ */
+	ss->ns_irq = platform_get_irq(pdev, 0);
+	if (ss->ns_irq < 0) {
+		dev_err(ss->dev, "Cannot get NS IRQ\n");
+		return ss->ns_irq;
+	}
+
+	err = devm_request_irq(&pdev->dev, ss->ns_irq, ce_irq_handler, 0,
+			       "sun8i-ce-ns", ss);
+	if (err < 0) {
+		dev_err(ss->dev, "Cannot request NS IRQ\n");
+		return err;
+	}
+
+	ss->reset = devm_reset_control_get_optional(&pdev->dev, "ahb");
+	if (IS_ERR(ss->reset)) {
+		if (PTR_ERR(ss->reset) == -EPROBE_DEFER)
+			return PTR_ERR(ss->reset);
+		dev_info(&pdev->dev, "no reset control found\n");
+		ss->reset = NULL;
+	}
+#ifdef SUN8I_CE_OVERVLOCK
+	err = clk_set_rate(ss->ssclk, 400 * 1000 * 1000);
+	dev_info(&pdev->dev, "clk_set_rate %d\n", err);
+#endif
+
+	err = clk_prepare_enable(ss->busclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable busclk\n");
+		return err;
+	}
+
+	err = clk_prepare_enable(ss->ssclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable ssclk\n");
+		goto error_clk;
+	}
+
+	err = reset_control_deassert(ss->reset);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot deassert reset control\n");
+		goto error_ssclk;
+	}
+
+	v = readl(ss->base + CE_CTR);
+	v >>= 16;
+	v &= 0x07;
+	dev_info(&pdev->dev, "CE_NS Die ID %x\n", v);
+
+	ss->dev = &pdev->dev;
+	platform_set_drvdata(pdev, ss);
+
+	mutex_init(&ss->mlock);
+
+	ss->chanlist = kcalloc(ss->variant->maxflow, sizeof(struct sun8i_ce_flow), GFP_KERNEL);
+	if (!ss->chanlist) {
+		err = -ENOMEM;
+		goto error_flow;
+	}
+
+	for (i = 0; i < ss->variant->maxflow; i++) {
+		init_completion(&ss->chanlist[i].complete);
+		mutex_init(&ss->chanlist[i].lock);
+
+		ss->chanlist[i].engine = crypto_engine_alloc_init(ss->dev, 1);
+		if (!ss->chanlist[i].engine) {
+			dev_err(ss->dev, "Cannot request engine\n");
+			goto error_engine;
+		}
+		err = crypto_engine_start(ss->chanlist[i].engine);
+		if (err) {
+			dev_err(ss->dev, "Cannot request engine\n");
+			goto error_engine;
+		}
+		ss->chanlist[i].tl = dma_alloc_coherent(ss->dev,
+							sizeof(struct ce_task),
+							&ss->chanlist[i].t_phy,
+							GFP_KERNEL);
+		if (!ss->chanlist[i].tl) {
+			dev_err(ss->dev, "Cannot get DMA memory for task %d\n",
+				i);
+			err = -ENOMEM;
+			goto error_engine;
+		}
+	}
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	ss->dbgfs_dir = debugfs_create_dir("sun8i-ce", NULL);
+	if (IS_ERR_OR_NULL(ss->dbgfs_dir)) {
+		dev_err(ss->dev, "Fail to create debugfs dir");
+		err = -ENOMEM;
+		goto error_engine;
+	}
+	ss->dbgfs_stats = debugfs_create_file("stats", 0444,
+		ss->dbgfs_dir, ss, &sun8i_ce_debugfs_fops);
+	if (IS_ERR_OR_NULL(ss->dbgfs_stats)) {
+		dev_err(ss->dev, "Fail to create debugfs stat");
+		err = -ENOMEM;
+		goto error_debugfs;
+	}
+#endif
+	for (i = 0; i < ARRAY_SIZE(ce_algs); i++) {
+		ce_algs[i].ss = ss;
+		switch (ce_algs[i].type) {
+		case CRYPTO_ALG_TYPE_SKCIPHER:
+			id = ce_algs[i].ce_algo_id;
+			ce_method = ss->variant->alg_cipher[id];
+			if (ce_method == CE_ID_NOTSUPP) {
+				dev_info(ss->dev, "DEBUG: Algo of %s not supp\n",
+					 ce_algs[i].alg.skcipher.base.cra_name);
+				ce_algs[i].ss = NULL;
+				break;
+			}
+			id = ce_algs[i].ce_blockmode;
+			ce_method = ss->variant->op_mode[id];
+			if (ce_method == CE_ID_NOTSUPP) {
+				dev_info(ss->dev, "DEBUG: Blockmode of %s not supp\n",
+					 ce_algs[i].alg.skcipher.base.cra_name);
+				ce_algs[i].ss = NULL;
+				break;
+			}
+			err = crypto_register_skcipher(&ce_algs[i].alg.skcipher);
+			if (err) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ce_algs[i].alg.skcipher.base.cra_name);
+				ce_algs[i].ss = NULL;
+				goto error_alg;
+			}
+			break;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+		case CRYPTO_ALG_TYPE_RNG:
+			ce_method = ss->variant->prng;
+			if (ce_method == CE_ID_NOTSUPP) {
+				ce_algs[i].ss = NULL;
+				break;
+			}
+			err = crypto_register_rng(&ce_algs[i].alg.rng);
+			if (err) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ce_algs[i].alg.rng.base.cra_name);
+				goto error_alg;
+			}
+			break;
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+		case CRYPTO_ALG_TYPE_AKCIPHER:
+			err = crypto_register_akcipher(&ce_algs[i].alg.rsa);
+			if (err != 0) {
+				dev_err(ss->dev, "Fail to register RSA %s\n",
+					ce_algs[i].alg.rsa.base.cra_name);
+				goto error_alg;
+			}
+			break;
+#endif
+		}
+	}
+
+	return 0;
+error_alg:
+	i--;
+	for (; i >= 0; i--) {
+		switch (ce_algs[i].type) {
+		case CRYPTO_ALG_TYPE_SKCIPHER:
+			if (ce_algs[i].ss)
+				crypto_unregister_skcipher(&ce_algs[i].alg.skcipher);
+			break;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+		case CRYPTO_ALG_TYPE_RNG:
+			if (ce_algs[i].ss)
+				crypto_unregister_rng(&ce_algs[i].alg.rng);
+			break;
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+		case CRYPTO_ALG_TYPE_AKCIPHER:
+			if (ce_algs[i].ss)
+				crypto_unregister_akcipher(&ce_algs[i].alg.rsa);
+			break;
+#endif
+		}
+	}
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+error_debugfs:
+	debugfs_remove_recursive(ss->dbgfs_dir);
+#endif
+error_engine:
+	while (i >= 0) {
+		crypto_engine_exit(ss->chanlist[i].engine);
+		if (ss->chanlist[i].tl)
+			dma_free_coherent(ss->dev, sizeof(struct ce_task),
+				ss->chanlist[i].tl, ss->chanlist[i].t_phy);
+		i--;
+	}
+	kfree(ss->chanlist);
+error_flow:
+	reset_control_assert(ss->reset);
+error_ssclk:
+	clk_disable_unprepare(ss->ssclk);
+error_clk:
+	clk_disable_unprepare(ss->busclk);
+	return err;
+}
+
+static int sun8i_ce_remove(struct platform_device *pdev)
+{
+	int i, timeout;
+	struct sun8i_ss_ctx *ss = platform_get_drvdata(pdev);
+
+	for (i = 0; i < ARRAY_SIZE(ce_algs); i++) {
+		switch (ce_algs[i].type) {
+		case CRYPTO_ALG_TYPE_SKCIPHER:
+			if (ce_algs[i].ss)
+				crypto_unregister_skcipher(&ce_algs[i].alg.skcipher);
+			break;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+		case CRYPTO_ALG_TYPE_RNG:
+			if (ce_algs[i].ss)
+				crypto_unregister_rng(&ce_algs[i].alg.rng);
+			break;
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+		case CRYPTO_ALG_TYPE_AKCIPHER:
+			if (ce_algs[i].ss)
+				crypto_unregister_akcipher(&ce_algs[i].alg.rsa);
+			break;
+#endif
+		}
+	}
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	debugfs_remove_recursive(ss->dbgfs_dir);
+#endif
+
+	for (i = 0; i < ss->variant->maxflow; i++) {
+		crypto_engine_exit(ss->chanlist[i].engine);
+		timeout = 0;
+		while (mutex_is_locked(&ss->chanlist[i].lock) && timeout < 10) {
+			dev_info(ss->dev, "Wait for %d %d\n", i, timeout);
+			timeout++;
+			msleep(20);
+		}
+	}
+
+	/* TODO check that any request are still under work */
+
+	reset_control_assert(ss->reset);
+	clk_disable_unprepare(ss->busclk);
+	return 0;
+}
+
+static const struct of_device_id sun8i_ce_crypto_of_match_table[] = {
+	{ .compatible = "allwinner,sun8i-h3-crypto",
+	  .data = &ce_h3_variant },
+	{ .compatible = "allwinner,sun50i-h5-crypto",
+	  .data = &ce_h5_variant },
+	{ .compatible = "allwinner,sun50i-a64-crypto",
+	  .data = &ce_a64_variant },
+	{ .compatible = "allwinner,sun8i-a83t-crypto",
+	  .data = &ce_a83t_variant },
+	{}
+};
+MODULE_DEVICE_TABLE(of, sun8i_ce_crypto_of_match_table);
+
+static struct platform_driver sun8i_ce_driver = {
+	.probe		 = sun8i_ce_probe,
+	.remove		 = sun8i_ce_remove,
+	.driver		 = {
+		.name		   = "sun8i-ce",
+		.of_match_table	= sun8i_ce_crypto_of_match_table,
+	},
+};
+
+module_platform_driver(sun8i_ce_driver);
+
+MODULE_DESCRIPTION("Allwinner Crypto Engine cryptographic accelerator");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Corentin Labbe <clabbe.montjoie@gmail.com>");
diff --git a/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-prng.c b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-prng.c
new file mode 100644
index 0000000000000..e089d569a32e8
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-prng.c
@@ -0,0 +1,152 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * sun8i-ce-prng.c - hardware cryptographic accelerator for
+ * Allwinner H3/A64/H5/H2+/H6/A80/A83T SoC
+ *
+ * Copyright (C) 2016-2017 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file adds support for the PRNG present in the CryptoEngine
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ */
+
+#include <crypto/internal/rng.h>
+#include "sun8i-ce.h"
+
+int sun8i_ce_prng_generate(struct crypto_rng *tfm, const u8 *src,
+			   unsigned int slen, u8 *dst, unsigned int dlen)
+{
+	struct sun8i_ce_prng_ctx *ctx = crypto_rng_ctx(tfm);
+	struct ce_task *cet;
+	int flow, ret = 0;
+	void *data;
+	size_t len;
+	int antifail = 0;
+	struct sun8i_ss_ctx *ss;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	struct rng_alg *alg = crypto_rng_alg(tfm);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.rng);
+	algt->stat_req++;
+#endif
+
+	ss = ctx->ss;
+	if (!ctx->seed) {
+		dev_err(ss->dev, "PRNG is un-seeded\n");
+		return -EINVAL;
+	}
+	dev_dbg(ss->dev, "%s %u %u\n", __func__, slen, dlen);
+
+	data = kmalloc(PRNG_DATA_SIZE, GFP_KERNEL | GFP_DMA);
+	if (!data)
+		return -ENOMEM;
+
+	flow = get_engine_number(ss);
+	mutex_lock(&ss->chanlist[flow].lock);
+	cet = ss->chanlist[flow].tl;
+	memset(cet, 0, sizeof(struct ce_task));
+	cet->t_id = flow;
+	cet->t_common_ctl = ctx->op | BIT(31);
+	cet->t_dlen = PRNG_DATA_SIZE / 4;
+	ss->chanlist[flow].op_mode = 0;
+	ss->chanlist[flow].op_dir = 0;
+	ss->chanlist[flow].method = ctx->op;
+
+/*	print_hex_dump(KERN_INFO, "RNG IV ", DUMP_PREFIX_NONE, 16, 1, ss->seed,
+		PRNG_SEED_SIZE, false);*/
+
+	ss->chanlist[flow].next_iv = kmalloc(PRNG_SEED_SIZE, GFP_KERNEL | GFP_DMA);
+	if (!ss->chanlist[flow].next_iv) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+rebegin:
+	len = min_t(size_t, dlen, PRNG_DATA_SIZE);
+	dev_dbg(ss->dev, "%s Rebegin %u dlen=%u steplen=%lu\n", __func__, slen, dlen, len);
+
+	cet->t_dst[0].addr = dma_map_single(ss->dev, data, PRNG_DATA_SIZE,
+					    DMA_FROM_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_dst[0].addr)) {
+		dev_err(ss->dev, "Cannot DMA MAP DST DATA\n");
+		ret = -EFAULT;
+		goto fail;
+	}
+	cet->t_dst[0].len = PRNG_DATA_SIZE / 4;
+
+	cet->t_key = cet->t_dst[0].addr;
+	cet->t_iv = dma_map_single(ss->dev, ctx->seed, PRNG_SEED_SIZE,
+				DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_iv)) {
+		dev_err(ss->dev, "Cannot DMA MAP SEED\n");
+		ret = -EFAULT;
+		goto ce_rng_iv_err;
+	}
+
+	ret = sun8i_ce_run_task(ss, flow, "PRNG");
+
+	dma_unmap_single(ss->dev, cet->t_iv, PRNG_SEED_SIZE, DMA_TO_DEVICE);
+ce_rng_iv_err:
+	dma_unmap_single(ss->dev, cet->t_dst[0].addr, PRNG_DATA_SIZE,
+			 DMA_FROM_DEVICE);
+fail:
+	memcpy(ctx->seed, ss->chanlist[flow].next_iv, PRNG_SEED_SIZE);
+/*	print_hex_dump(KERN_INFO, "RNG NIV ", DUMP_PREFIX_NONE, 16, 1, ss->seed,
+		PRNG_SEED_SIZE, false);*/
+
+	if (!ret) {
+		memcpy(dst, data, len);
+		dst += len;
+		dlen -= len;
+		if (dlen > 4 && antifail++ < 10)
+			goto rebegin;
+	}
+
+	kfree(ss->chanlist[flow].next_iv);
+	mutex_unlock(&ss->chanlist[flow].lock);
+	memzero_explicit(data, PRNG_DATA_SIZE);
+	kfree(data);
+
+	return ret;
+}
+
+int sun8i_ce_prng_seed(struct crypto_rng *tfm, const u8 *seed,
+		       unsigned int slen)
+{
+	struct sun8i_ce_prng_ctx *ctx = crypto_rng_ctx(tfm);
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	struct rng_alg *alg = crypto_rng_alg(tfm);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.rng);
+#endif
+	if (slen < PRNG_SEED_SIZE) {
+		dev_err(ctx->ss->dev, "ERROR: Invalid seedsize get %u instead of %u\n", slen, PRNG_SEED_SIZE);
+		return -EINVAL;
+	}
+	if (!ctx->seed)
+		ctx->seed = kmalloc(slen, GFP_KERNEL | GFP_DMA);
+	if (!ctx->seed)
+		return -ENOMEM;
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	algt->stat_fb++;
+#endif
+	memcpy(ctx->seed, seed, PRNG_SEED_SIZE);
+
+	return 0;
+}
+
+int sun8i_ce_prng_init(struct crypto_tfm *tfm)
+{
+	struct sun8i_ce_prng_ctx *ctx = crypto_tfm_ctx(tfm);
+	struct sun8i_ss_alg_template *algt;
+	struct crypto_rng *rngtfm = __crypto_rng_cast(tfm);
+	struct rng_alg *alg = crypto_rng_alg(rngtfm);
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.rng);
+	ctx->ss = algt->ss;
+	ctx->op = ctx->ss->variant->prng;
+
+	return 0;
+}
diff --git a/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-rsa.c b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-rsa.c
new file mode 100644
index 0000000000000..3ec5c7e9b0c0c
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-rsa.c
@@ -0,0 +1,470 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * sun8i-ce-cipher.c - hardware cryptographic accelerator for
+ * Allwinner H3/A64/H5/H2+/H6/A80/A83T SoC
+ *
+ * Copyright (C) 2016-2018 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for RSA operations
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ */
+#include <linux/crypto.h>
+#include <linux/module.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <crypto/internal/akcipher.h>
+#include <crypto/internal/rsa.h>
+#include <linux/dma-mapping.h>
+#include "sun8i-ce.h"
+
+/* The data should be presented in a form of array of the key size
+ * (modulus, key, data) such as : [LSB....MSB]
+ * and the result will be return following the same pattern
+ * the key (exposant) buffer is not reversed [MSB...LSB]
+ * (in contrary to other data such as modulus and encryption buffer
+ */
+static int sun8i_rsa_operation(struct akcipher_request *req, int dir);
+
+static int handle_rsa_request(struct crypto_engine *engine,
+			      void *areq)
+{
+	int err;
+	struct akcipher_request *req = container_of(areq, struct akcipher_request, base);
+	struct sun8i_rsa_req_ctx *rsa_req_ctx = akcipher_request_ctx(req);
+	int opdir;
+
+	opdir = rsa_req_ctx->op_dir;
+
+	err = sun8i_rsa_operation(req, opdir);
+	crypto_finalize_akcipher_request(engine, req, err);
+	return 0;
+}
+
+int sun8i_rsa_init(struct crypto_akcipher *tfm)
+{
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	struct akcipher_alg *alg = crypto_akcipher_alg(tfm);
+	struct sun8i_ss_alg_template *algt;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.rsa);
+	ctx->ss = algt->ss;
+
+	dev_info(ctx->ss->dev, "%s\n", __func__);
+
+	ctx->fallback = crypto_alloc_akcipher("rsa", 0, CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(ctx->fallback)) {
+		dev_err(ctx->ss->dev, "ERROR: Cannot allocate fallback\n");
+		return PTR_ERR(ctx->fallback);
+	}
+	/*dev_info(ctx->ss->dev, "Use %s as fallback\n", ctx->fallback->base.cra_driver_name);*/
+
+	akcipher_set_reqsize(tfm, sizeof(struct sun8i_rsa_req_ctx));
+
+	ctx->enginectx.op.do_one_request = handle_rsa_request;
+	ctx->enginectx.op.prepare_request = NULL;
+	ctx->enginectx.op.unprepare_request = NULL;
+	return 0;
+}
+
+void sun8i_rsa_exit(struct crypto_akcipher *tfm)
+{
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+
+	dev_info(ctx->ss->dev, "%s\n", __func__);
+	crypto_free_akcipher(ctx->fallback);
+
+	kfree(ctx->rsa_priv_key);
+}
+
+static inline u8 *caam_read_raw_data(const u8 *buf, size_t *nbytes)
+{
+	u8 *val;
+
+	while (!*buf && *nbytes) {
+		buf++;
+		(*nbytes)--;
+	}
+
+	val = kzalloc(*nbytes, GFP_DMA | GFP_KERNEL);
+	if (!val)
+		return NULL;
+
+	memcpy(val, buf, *nbytes);
+	return val;
+}
+
+static void padd(u8 *src, size_t len, u8 *tmp)
+{
+	int i;
+
+	/*pr_info("padd %zd\n", len);*/
+	memcpy(tmp, src, len);
+	for (i = 0; i < len; i++)
+		src[i] = tmp[len - i - 1];
+}
+
+/* IV is pubmodulus
+ *
+ * mode MUL(2) IV size
+ * mode EXP(0) key size
+ * TODO check align
+ */
+static int sun8i_rsa_operation(struct akcipher_request *req, int dir)
+{
+	struct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	int flow = 0;
+	struct ce_task *cet;
+	struct sun8i_ss_ctx *ss = ctx->ss;
+	int err = 0;
+	u8 *modulus = NULL;
+	int nr_sgd;
+	int i;
+	unsigned int todo, len;
+	struct scatterlist *sg;
+	void *sgb = NULL, *key = NULL, *tmp = NULL;
+	u8 *s, *t;
+	struct akcipher_request *freq;
+	size_t blk_size;
+	struct akcipher_alg *alg = crypto_akcipher_alg(tfm);
+	struct sun8i_ss_alg_template *algt;
+	bool need_fallback = false;
+
+	algt = container_of(alg, struct sun8i_ss_alg_template, alg.rsa);
+
+	dev_info(ctx->ss->dev, "%s modulus %zu e=%zu d=%zu c=%zu slen=%u dlen=%u\n",
+		 __func__,
+		 ctx->rsa_key.n_sz, ctx->rsa_key.e_sz, ctx->rsa_key.d_sz,
+		 ctx->rsa_key.n_sz,
+		 req->src_len, req->dst_len);
+
+	cet = ctx->ss->chanlist[flow].tl;
+	memset(cet, 0, sizeof(struct ce_task));
+
+	cet->t_id = flow;
+	cet->t_common_ctl = ss->variant->alg_akcipher[algt->ce_algo_id] | CE_COMM_INT;
+	ctx->ss->chanlist[flow].method = ss->variant->alg_akcipher[algt->ce_algo_id];
+#define RSA_LENDIV 4
+
+	blk_size = ctx->rsa_key.n_sz;
+	modulus = caam_read_raw_data(ctx->rsa_key.n, &blk_size);
+	if (!modulus) {
+		dev_err(ss->dev, "Cannot get modulus\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	dev_dbg(ss->dev, "Final modulus size %zu (RSA %zu)\n", blk_size,
+		blk_size * 8);
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	algt->stat_req++;
+#endif
+
+	switch (blk_size * 8) {
+	case 512:
+		cet->t_asym_ctl = ss->variant->rsa_op_mode[CE_ID_RSA_512];
+		dev_info(ss->dev, "RSA 512\n");
+		break;
+	case 1024:
+		cet->t_asym_ctl = ss->variant->rsa_op_mode[CE_ID_RSA_1024];
+		dev_info(ss->dev, "RSA 1024\n");
+		break;
+	case 2048:
+		cet->t_asym_ctl = ss->variant->rsa_op_mode[CE_ID_RSA_2048];
+		dev_info(ss->dev, "RSA 2048\n");
+		break;
+	case 3072:
+		cet->t_asym_ctl = ss->variant->rsa_op_mode[CE_ID_RSA_3072];
+		dev_info(ss->dev, "RSA 3072\n");
+		break;
+	case 4096:
+		cet->t_asym_ctl = ss->variant->rsa_op_mode[CE_ID_RSA_4096];
+		dev_info(ss->dev, "RSA 4096\n");
+		break;
+	default:
+		dev_info(ss->dev, "RSA invalid keysize\n");
+		/* TODO */
+	}
+	ctx->ss->chanlist[flow].op_mode = cet->t_asym_ctl;
+	if (cet->t_asym_ctl == CE_ID_NOTSUPP) {
+		dev_info(ss->dev, "Unsupported size\n");
+		need_fallback = true;
+	}
+
+	/* check if fallback is necessary */
+	if (req->src_len != blk_size ||
+	    blk_size > ss->variant->maxrsakeysize / 8 ||
+	    need_fallback ||
+	    (dir == CE_DECRYPTION && blk_size * 8 == 1024)) {
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+		algt->stat_fb++;
+#endif
+		dev_info(ss->dev, "Fallback %d %zd (keylen=%d)\n",
+			 req->src_len, blk_size, ctx->key_len);
+		if (ctx->rsa_priv_key) {
+			err = crypto_akcipher_set_priv_key(ctx->fallback,
+							   ctx->rsa_priv_key,
+							   ctx->key_len);
+		} else if (ctx->rsa_pub_key) {
+			err = crypto_akcipher_set_pub_key(ctx->fallback,
+							  ctx->rsa_pub_key,
+							  ctx->key_len);
+		} else {
+			dev_err(ss->dev, "ERROR: no private or public key given\n");
+			err = -EINVAL;
+		}
+		if (err)
+			return err;
+
+		freq = akcipher_request_alloc(ctx->fallback, GFP_KERNEL);
+		if (!freq)
+			return -ENOMEM;
+		req->dst_len = blk_size;
+		akcipher_request_set_crypt(freq, req->src, req->dst,
+					   req->src_len, req->dst_len);
+		if (dir == CE_DECRYPTION)
+			err = crypto_akcipher_decrypt(freq);
+		else
+			err = crypto_akcipher_encrypt(freq);
+		if (err)
+			return err;
+		/*dev_info(ss->dev, "Fallback end %d\n", err);*/
+		/* hack fix max_size func*/
+		req->dst_len = blk_size;
+		akcipher_request_free(freq);
+		return 0;
+	}
+
+	tmp = kzalloc(blk_size, GFP_KERNEL | GFP_DMA);
+	if (!tmp)
+		return -ENOMEM;
+	key = kzalloc(blk_size, GFP_KERNEL | GFP_DMA);
+	if (!key)
+		return -ENOMEM;
+	/* key is exponant(encrypt) or d(decrypt) */
+	if (dir == CE_ENCRYPTION) {
+		memcpy(key, ctx->rsa_key.e, ctx->rsa_key.e_sz);
+#ifdef DEBUG_CE_HEX
+		print_hex_dump(KERN_INFO, "EXP ", DUMP_PREFIX_NONE, 16, 1, key,
+			       blk_size, false);
+#endif
+	} else {
+		memcpy(key, ctx->rsa_key.d, ctx->rsa_key.d_sz);
+		padd(key, blk_size, tmp);
+		cet->t_common_ctl |= CE_DECRYPTION;
+	}
+
+	/* exposant set as key */
+	cet->t_key = dma_map_single(ss->dev, key, blk_size, DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_key)) {
+		dev_err(ss->dev, "Cannot DMA MAP KEY\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	/* invert modulus */
+	memcpy(tmp, modulus, blk_size);
+	padd(modulus, blk_size, tmp);
+
+	/*check_align(modulus);*/
+	/* modulus set as IV */
+	cet->t_iv = dma_map_single(ss->dev, modulus, blk_size, DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_iv)) {
+		dev_err(ss->dev, "Cannot DMA MAP IV\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+#ifdef DEBUG_CE_HEX
+	print_hex_dump(KERN_INFO, "KEY ", DUMP_PREFIX_NONE, 16, 1,
+		       ctx->rsa_key.e, ctx->rsa_key.e_sz, false);
+	print_hex_dump(KERN_INFO, "MOD ", DUMP_PREFIX_NONE, 16, 1, modulus,
+		       blk_size, false);
+#endif
+	/* handle data */
+	sgb = kzalloc(blk_size, GFP_KERNEL | GFP_DMA);
+	if (!sgb)
+		return -ENOMEM;
+	err = sg_copy_to_buffer(req->src, sg_nents(req->src), sgb,
+				req->src_len);
+	/* invert src */
+	padd(sgb, blk_size, tmp);
+
+	/*check_align(sgb);*/
+#ifdef DEBUG_CE_HEX
+	print_hex_dump(KERN_INFO, "SRC ", DUMP_PREFIX_NONE, 16, 1, sgb,
+		       blk_size, false);
+#endif
+	cet->t_src[0].addr = dma_map_single(ss->dev, sgb, blk_size,
+					    DMA_TO_DEVICE);
+	if (dma_mapping_error(ss->dev, cet->t_src[0].addr)) {
+		dev_err(ss->dev, "Cannot DMA MAP SRC\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	/* handle destination data */
+	nr_sgd = dma_map_sg(ss->dev, req->dst, sg_nents(req->dst),
+			    DMA_FROM_DEVICE);
+	if (nr_sgd < 0) {
+		dev_err(ss->dev, "Cannot DMA MAP dst\n");
+		err = -EFAULT;
+		goto theend;
+	}
+
+	req->dst_len = blk_size;
+	len = blk_size;
+	for_each_sg(req->dst, sg, nr_sgd, i) {
+		cet->t_dst[i].addr = sg_dma_address(sg);
+		todo = min(len, sg_dma_len(sg));
+		cet->t_dst[i].len = todo / RSA_LENDIV;
+		dev_info(ss->dev, "DST %02d todo=%u\n", i, todo);
+		len -= todo;
+	}
+	/*check_align(cet->t_dst[0].addr);*/
+
+	cet->t_src[0].len = blk_size / RSA_LENDIV;
+	cet->t_dlen = blk_size / RSA_LENDIV;
+
+	dev_info(ss->dev, "SRC %u\n", cet->t_src[0].len);
+	/*dev_info(ss->dev, "DST %u\n", cet->t_dst[0].len);*/
+	dev_info(ss->dev, "CTL %x %x %x\n", cet->t_common_ctl, cet->t_sym_ctl,
+		 cet->t_asym_ctl);
+
+	err = sun8i_ce_run_task(ss, flow, "RSA");
+
+	dma_unmap_single(ss->dev, cet->t_src[0].addr, blk_size, DMA_TO_DEVICE);
+	dma_unmap_sg(ss->dev, req->dst, nr_sgd, DMA_FROM_DEVICE);
+	dma_unmap_single(ss->dev, cet->t_key, blk_size, DMA_TO_DEVICE);
+	dma_unmap_single(ss->dev, cet->t_iv, blk_size, DMA_TO_DEVICE);
+
+	sg_copy_to_buffer(req->dst, sg_nents(req->dst), modulus, req->dst_len);
+
+	/* invert DST */
+	t = modulus;
+	s = sgb;
+	for (i = 0; i < blk_size; i++)
+		s[i] = t[blk_size - i - 1];
+	sg_copy_from_buffer(req->dst, sg_nents(req->dst), sgb, req->dst_len);
+
+theend:
+	kfree(modulus);
+	kfree(tmp);
+	return err;
+}
+
+int sun8i_rsa_encrypt(struct akcipher_request *req)
+{
+	struct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	struct sun8i_rsa_req_ctx *rsa_req_ctx = akcipher_request_ctx(req);
+	int e = get_engine_number(ctx->ss);
+	struct crypto_engine *engine = ctx->ss->chanlist[e].engine;
+
+	dev_info(ctx->ss->dev, "%s\n", __func__);
+	rsa_req_ctx->op_dir = CE_ENCRYPTION;
+	return crypto_transfer_akcipher_request_to_engine(engine, req);
+
+	return sun8i_rsa_operation(req, CE_ENCRYPTION);
+}
+
+int sun8i_rsa_decrypt(struct akcipher_request *req)
+{
+	struct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	struct sun8i_rsa_req_ctx *rsa_req_ctx = akcipher_request_ctx(req);
+	int e = get_engine_number(ctx->ss);
+	struct crypto_engine *engine = ctx->ss->chanlist[e].engine;
+
+	dev_info(ctx->ss->dev, "%s modulus %zu e=%zu d=%zu c=%zu slen=%u dlen=%u\n",
+		 __func__,
+		 ctx->rsa_key.n_sz, ctx->rsa_key.e_sz, ctx->rsa_key.d_sz,
+		 ctx->rsa_key.n_sz,
+		 req->src_len, req->dst_len);
+	rsa_req_ctx->op_dir = CE_DECRYPTION;
+	return crypto_transfer_akcipher_request_to_engine(engine, req);
+
+	return sun8i_rsa_operation(req, CE_DECRYPTION);
+}
+
+int sun8i_rsa_sign(struct akcipher_request *req)
+{
+	pr_info("%s un-implemented\n", __func__);
+	return 0;
+}
+
+int sun8i_rsa_verify(struct akcipher_request *req)
+{
+	struct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	struct sun8i_rsa_req_ctx *rsa_req_ctx = akcipher_request_ctx(req);
+	int e = get_engine_number(ctx->ss);
+	struct crypto_engine *engine = ctx->ss->chanlist[e].engine;
+
+	dev_info(ctx->ss->dev, "%s\n", __func__);
+	rsa_req_ctx->op_dir = CE_ENCRYPTION;
+	return crypto_transfer_akcipher_request_to_engine(engine, req);
+
+	sun8i_rsa_operation(req, CE_ENCRYPTION);
+	return 0;
+}
+
+int sun8i_rsa_set_priv_key(struct crypto_akcipher *tfm, const void *key,
+			   unsigned int keylen)
+{
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	int ret;
+
+	dev_info(ctx->ss->dev, "%s keylen=%u\n", __func__, keylen);
+
+	kfree(ctx->rsa_pub_key);
+	ctx->rsa_pub_key = NULL;
+	ctx->rsa_priv_key = kmalloc(keylen, GFP_KERNEL);
+	if (!ctx->rsa_priv_key)
+		return -ENOMEM;
+	memcpy(ctx->rsa_priv_key, key, keylen);
+	ctx->key_len = keylen;
+
+	ret = rsa_parse_priv_key(&ctx->rsa_key, key, keylen);
+	if (ret) {
+		dev_err(ctx->ss->dev, "Invalid private key\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+int sun8i_rsa_set_pub_key(struct crypto_akcipher *tfm, const void *key,
+			  unsigned int keylen)
+{
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+	int ret;
+
+	dev_info(ctx->ss->dev, "%s keylen=%u\n", __func__, keylen);
+
+	kfree(ctx->rsa_priv_key);
+	ctx->rsa_priv_key = NULL;
+	ctx->rsa_pub_key = kmalloc(keylen, GFP_KERNEL);
+	if (!ctx->rsa_pub_key)
+		return -ENOMEM;
+	memcpy(ctx->rsa_pub_key, key, keylen);
+	ctx->key_len = keylen;
+
+	memset(&ctx->rsa_key, 0, sizeof(struct rsa_key));
+	ret = rsa_parse_pub_key(&ctx->rsa_key, key, keylen);
+	if (ret) {
+		dev_err(ctx->ss->dev, "Invalid public key\n");
+		return ret;
+	}
+	return 0;
+}
+
+unsigned int sun8i_rsa_max_size(struct crypto_akcipher *tfm)
+{
+	struct sun8i_tfm_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);
+
+	return ctx->key_len;
+}
diff --git a/drivers/crypto/allwinner/sun8i-ce/sun8i-ce.h b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce.h
new file mode 100644
index 0000000000000..76c84bd1550fe
--- /dev/null
+++ b/drivers/crypto/allwinner/sun8i-ce/sun8i-ce.h
@@ -0,0 +1,332 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * sun8i-ce.h - hardware cryptographic accelerator for
+ * Allwinner H3/A64/H5/H2+/H6/A80/A83T SoC
+ *
+ * Copyright (C) 2016-2018 Corentin LABBE <clabbe.montjoie@gmail.com>
+ */
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/engine.h>
+#include <crypto/rng.h>
+#include <crypto/akcipher.h>
+#include <crypto/skcipher.h>
+#include <crypto/internal/rsa.h>
+#include <linux/debugfs.h>
+#include <linux/crypto.h>
+#include <linux/scatterlist.h>
+
+/* CE Registers */
+#define CE_TDQ	0x00
+#define CE_CTR	0x04
+#define CE_ICR	0x08
+#define CE_ISR	0x0C
+#define CE_TLR	0x10
+#define CE_TSR	0x14
+#define CE_ESR	0x18
+#define CE_CSSGR	0x1C
+#define CE_CDSGR	0x20
+#define CE_CSAR	0x24
+#define CE_CDAR	0x28
+#define CE_TPR	0x2C
+
+/* Operation direction */
+#define SS_ENCRYPTION		0
+#define SS_DECRYPTION		BIT(6)
+#define CE_ENCRYPTION		0
+#define CE_DECRYPTION		BIT(8)
+
+/* CE Method H3/A64 */
+#define CE_ALG_AES		0
+#define CE_ALG_DES		1
+#define CE_ALG_3DES		2
+#define CE_ALG_MD5		16
+#define CE_ALG_SHA1		17
+#define CE_ALG_SHA224		18
+#define CE_ALG_SHA256		19
+#define CE_ALG_SHA384		20
+#define CE_ALG_SHA512		21
+#define CE_ALG_RSA		32
+#define CE_ALG_TRNG		48
+#define CE_ALG_PRNG		49
+
+#define CE_COMM_INT		BIT(31)
+
+/* SS Method A83T */
+#define SS_ALG_AES		0
+#define SS_ALG_DES		(1 << 2)
+#define SS_ALG_3DES		(2 << 2)
+#define SS_ALG_MD5		(3 << 2)
+#define SS_ALG_PRNG		(4 << 2)
+#define SS_ALG_TRNG		(5 << 2)
+#define SS_ALG_SHA1		(6 << 2)
+#define SS_ALG_SHA224		(7 << 2)
+#define SS_ALG_SHA256		(8 << 2)
+#define SS_ALG_RSA		(9 << 2)
+
+/* A80/A83T SS Registers */
+#define SS_CTL_REG		0x00
+#define SS_INT_CTL_REG		0x04
+#define SS_INT_STA_REG		0x08
+#define SS_KEY_ADR_REG		0x10
+#define SS_IV_ADR_REG		0x18
+#define SS_SRC_ADR_REG		0x20
+#define SS_DST_ADR_REG		0x28
+#define SS_LEN_ADR_REG		0x30
+#define SS_CTR_REG0	0x34
+#define SS_CTR_REG1	0x48
+
+#define CE_ID_NOTSUPP		0xFF
+
+#define CE_ID_CIPHER_AES	1
+#define CE_ID_CIPHER_DES	2
+#define CE_ID_CIPHER_DES3	3
+#define CE_ID_CIPHER_MAX	4
+
+#define CE_ID_OP_ECB	1
+#define CE_ID_OP_CBC	2
+#define CE_ID_OP_CTR	3
+#define CE_ID_OP_CTS	4
+#define CE_ID_OP_OFB	5
+#define CE_ID_OP_CFB	6
+#define CE_ID_OP_CBCMAC	7
+#define CE_ID_OP_MAX	8
+
+#define CE_AES_128BITS 0
+#define CE_AES_192BITS 1
+#define CE_AES_256BITS 2
+
+#define CE_OP_ECB	0
+#define CE_OP_CBC	(1 << 8)
+#define CE_OP_CTR	(2 << 8)
+#define CE_OP_CTS	(3 << 8)
+
+#define SS_OP_ECB	0
+#define SS_OP_CBC	(1 << 13)
+#define SS_OP_CTR	(2 << 14)
+#define SS_OP_CTS	(3 << 14)
+
+#define CE_CTR_128	(3 << 2)
+#define SS_CTR_128	(3 << 11)
+#define CE_CTS		BIT(16)
+
+#define CE_ID_AKCIPHER_RSA 1
+#define CE_ID_AKCIPHER_MAX 2
+
+#define CE_ID_RSA_512	0
+#define CE_ID_RSA_1024	1
+#define CE_ID_RSA_2048	2
+#define CE_ID_RSA_3072	3
+#define CE_ID_RSA_4096	4
+#define CE_ID_RSA_MAX	5
+
+#define CE_OP_RSA_512	0
+#define CE_OP_RSA_1024	(1 << 28)
+#define CE_OP_RSA_2048	(2 << 28)
+#define CE_OP_RSA_3072	(3 << 28)
+#define CE_OP_RSA_4096	(4 << 28)
+
+#define SS_OP_RSA_512	0
+#define SS_OP_RSA_1024	(1 << 9)
+#define SS_OP_RSA_2048	(2 << 9)
+#define SS_OP_RSA_3072	(3 << 9)
+
+#define SS_FLOW0	BIT(30)
+#define SS_FLOW1	BIT(31)
+
+#define SS_RNG_CONTINUE	BIT(18)
+
+#define TRNG_DATA_SIZE (256 / 8)
+#define PRNG_DATA_SIZE (160 / 8)
+#define PRNG_SEED_SIZE DIV_ROUND_UP(175, 8)
+
+#define CE_ARBIT_IV	BIT(16)
+#define SS_ARBIT_IV	BIT(17)
+
+#define MAXCHAN 4
+#define MAX_SG 8
+
+/* struct ce_variant - Describe CE capability for each variant hardware
+ * @alg_cipher:	list of supported ciphers
+ * @op_mode:	list of supported block modes
+ * @is_ss:	True if the hardware is SecuritySystem
+ * @intreg:	reg offset for Interrupt register
+ * @maxflow:	Numbers of flow for the current engine
+ */
+struct ce_variant {
+	char alg_cipher[CE_ID_CIPHER_MAX];
+	u32 op_mode[CE_ID_OP_MAX];
+	bool is_ss;
+	u32 intreg;
+	unsigned int maxflow;
+	char prng;
+	unsigned int maxrsakeysize;
+	char alg_akcipher[CE_ID_AKCIPHER_MAX];
+	u32 rsa_op_mode[CE_ID_RSA_MAX];
+};
+
+struct sginfo {
+	u32 addr;
+	u32 len;
+} __packed;
+
+struct ce_task {
+	u32 t_id;
+	u32 t_common_ctl;
+	u32 t_sym_ctl;
+	u32 t_asym_ctl;
+	u32 t_key;
+	u32 t_iv;
+	u32 t_ctr;
+	u32 t_dlen;
+	struct sginfo t_src[MAX_SG];
+	struct sginfo t_dst[MAX_SG];
+	u32 next;
+	u32 reserved[3];
+} __packed __aligned(8);
+
+/* struct sun8i_ce_flow - Information used by each flow
+ * @status:	set to 1 by interrupt
+ * @t_phy:	Physical address of task
+ */
+struct sun8i_ce_flow {
+	/* TODO comment on lock */
+	struct mutex lock;
+	struct crypto_engine *engine;
+	/* IV to use */
+	void *bounce_iv;
+	void *next_iv;
+	unsigned int ivlen;
+	struct completion complete;
+	int status;
+	u32 method;
+	u32 op_dir;
+	u32 op_mode;
+	unsigned int keylen;
+	/* number of SG to handle in this channel */
+	int nbsg;
+	dma_addr_t t_phy;
+	struct ce_task *tl;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	unsigned long stat_req;
+#endif
+};
+
+struct sun8i_ss_ctx {
+	void __iomem *base;
+	void __iomem *nsbase;
+	int ns_irq;
+	struct clk *busclk;
+	struct clk *ssclk;
+	struct reset_control *reset;
+	struct device *dev;
+	struct resource *res;
+	struct mutex mlock; /* control the use of the device */
+	struct sun8i_ce_flow *chanlist;
+	int flow; /* flow to use in next request */
+	const struct ce_variant *variant;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	struct dentry *dbgfs_dir;
+	struct dentry *dbgfs_stats;
+#endif
+};
+
+struct sun8i_cipher_req_ctx {
+	u32 op_dir;
+	int flow;
+};
+
+struct sun8i_tfm_ctx {
+	struct crypto_engine_ctx enginectx;
+	u32 *key;
+	u32 keylen;
+	u32 keymode;
+	struct sun8i_ss_ctx *ss;
+	struct crypto_skcipher *fallback_tfm;
+};
+
+struct sun8i_tfm_rsa_ctx {
+	struct crypto_engine_ctx enginectx;
+	struct sun8i_ss_ctx *ss;
+	struct rsa_key rsa_key;
+	/* used for fallback */
+	struct crypto_akcipher *fallback;
+	void *rsa_priv_key;
+	void *rsa_pub_key;
+	unsigned int key_len;
+};
+
+struct sun8i_rsa_req_ctx {
+	u32 op_dir;
+	int flow;
+};
+
+/*
+ * struct sun8i_ce_prng_ctx - Store data for PRNG operations
+ * @ss:		TODO
+ * @seed:	TODO
+ * @op:		TODO
+*/
+struct sun8i_ce_prng_ctx {
+	struct sun8i_ss_ctx *ss;
+	void *seed;
+	u32 op;
+};
+
+struct sun8i_ss_alg_template {
+	u32 type;
+	u32 mode;
+	u32 ce_algo_id;
+	u32 ce_blockmode;
+	const void *hash_init;
+	union {
+		struct skcipher_alg skcipher;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+		struct rng_alg rng;
+#endif
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+		struct akcipher_alg rsa;
+#endif
+	} alg;
+	struct sun8i_ss_ctx *ss;
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG
+	unsigned long stat_req;
+	unsigned long stat_fb;
+#endif
+};
+
+int sun8i_ce_enqueue(struct crypto_async_request *areq, u32 type);
+
+int sun8i_ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			unsigned int keylen);
+int sun8i_ce_des3_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			unsigned int keylen);
+int sun8i_ce_cipher_init(struct crypto_tfm *tfm);
+void sun8i_ce_cipher_exit(struct crypto_tfm *tfm);
+int sun8i_ce_skdecrypt(struct skcipher_request *areq);
+int sun8i_ce_skencrypt(struct skcipher_request *areq);
+
+int get_engine_number(struct sun8i_ss_ctx *ss);
+
+int sun8i_ce_run_task(struct sun8i_ss_ctx *ss, int flow, const char *name);
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_PRNG
+int sun8i_ce_prng_generate(struct crypto_rng *tfm, const u8 *src,
+			   unsigned int slen, u8 *dst, unsigned int dlen);
+int sun8i_ce_prng_seed(struct crypto_rng *tfm, const u8 *seed, unsigned int slen);
+int sun8i_ce_prng_init(struct crypto_tfm *tfm);
+#endif
+
+#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_RSA
+int sun8i_rsa_encrypt(struct akcipher_request *req);
+int sun8i_rsa_decrypt(struct akcipher_request *req);
+int sun8i_rsa_sign(struct akcipher_request *req);
+int sun8i_rsa_verify(struct akcipher_request *req);
+int sun8i_rsa_set_priv_key(struct crypto_akcipher *tfm, const void *key,
+			   unsigned int keylen);
+int sun8i_rsa_set_pub_key(struct crypto_akcipher *tfm, const void *key,
+			  unsigned int keylen);
+unsigned int sun8i_rsa_max_size(struct crypto_akcipher *tfm);
+int sun8i_rsa_init(struct crypto_akcipher *tfm);
+void sun8i_rsa_exit(struct crypto_akcipher *tfm);
+#endif
